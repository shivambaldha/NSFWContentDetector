{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, we developed an OCR API designed to extract text from images. For this purpose, we leveraged an open-source model available through Hugging Face.\n",
    "\n",
    "# Model Info\n",
    "\n",
    "The model used is **Nanonets-OCR**, a state-of-the-art image-to-markdown OCR system that goes well beyond basic text extraction. Unlike traditional approaches, this advanced model converts documents into structured markdown with intelligent content recognition and semantic tagging.\n",
    "\n",
    "Most publicly available image-to-text models are limited to plain text extraction. They often fail to differentiate between key elements such as watermarks, signatures, or page numbers.\n",
    "\n",
    "In our use case, the provided images are vibrant and colorful, which makes it difficult for conventional OCR tools to capture the text accurately. This is why we chose Nanonets-OCR — it is capable of extracting text effectively even from complex, colorful images.\n",
    "\n",
    "For more information about the model, see: [Nanonets-OCR](https://nanonets.com/research/nanonets-ocr-s/)\n",
    "\n",
    "# Use\n",
    "\n",
    "We integrated this model into a simple **Flask API**, which is then used in the next stage of our pipeline. Since the model runs best on a GPU, we created the API to ensure faster performance and seamless integration.\n",
    "\n",
    "For setup instructions, please refer to the README file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T17:17:10.025638Z",
     "iopub.status.busy": "2025-10-02T17:17:10.025368Z",
     "iopub.status.idle": "2025-10-02T17:17:15.861695Z",
     "shell.execute_reply": "2025-10-02T17:17:15.860675Z",
     "shell.execute_reply.started": "2025-10-02T17:17:10.025617Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyngrok\n",
      "  Downloading pyngrok-7.4.0-py3-none-any.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
      "Downloading pyngrok-7.4.0-py3-none-any.whl (25 kB)\n",
      "Installing collected packages: pyngrok\n",
      "Successfully installed pyngrok-7.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyngrok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ngrok Token Setup\n",
    "\n",
    "Replace the placeholder token in the URL with your actual **ngrok authtoken**:\n",
    "\n",
    "https://dashboard.ngrok.com/get-started/your-authtoken\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T17:17:15.863389Z",
     "iopub.status.busy": "2025-10-02T17:17:15.863140Z",
     "iopub.status.idle": "2025-10-02T17:17:17.095181Z",
     "shell.execute_reply": "2025-10-02T17:17:17.094447Z",
     "shell.execute_reply.started": "2025-10-02T17:17:15.863360Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml                                \n"
     ]
    }
   ],
   "source": [
    "!ngrok config add-authtoken <YOUR_NGROK_AUTH_TOKEN>  # Replace this token with your token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T17:17:17.096233Z",
     "iopub.status.busy": "2025-10-02T17:17:17.096013Z",
     "iopub.status.idle": "2025-10-02T17:18:59.248953Z",
     "shell.execute_reply": "2025-10-02T17:18:59.247710Z",
     "shell.execute_reply.started": "2025-10-02T17:17:17.096211Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-02 17:17:37.238923: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1759425457.598081      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1759425457.705817      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf5d9ff864174bf290224f91adef4e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70492f9ee3484d44b56ce04d3c01dcd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d72fa3639e40f98be41efeb505e4d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7adc60f4dad644f1b6d6eca34e0ae886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.51G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6779e3adc6d24fb9bd027c7cde809366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22844a7440db4a3aa712f889695283bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8564d3d4271349b196a7dfb79b2cc43d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/214 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82844b473da041b2ba5f3c3a75399fd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9891f188e21146ac8caf34fded90bfd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08513a51d4dd488798b408cd8e890ff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c97cd5fa428e41b4968e95aa699394e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "725734e61d4040cd81eb4adfd5f2a461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78d6419971374e19a95e574723623477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/613 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f56ecdf211e455eaebdeeb49af0d5a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "602d68cff3c545d88ef1a5477bd36b6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/575 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a96ed7c476b1402ba7e1d6f7cf6517eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "video_preprocessor_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from PIL import Image\n",
    "from transformers import AutoTokenizer, AutoProcessor, AutoModelForImageTextToText\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Load model and processor once when app starts\n",
    "model_path = \"nanonets/Nanonets-OCR-s\"\n",
    "\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "processor = AutoProcessor.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-02T18:58:44.234Z",
     "iopub.execute_input": "2025-10-02T17:18:59.252041Z",
     "iopub.status.busy": "2025-10-02T17:18:59.251180Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32ffe5841c8342628e2333a79ae6e1a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * ngrok tunnel available at NgrokTunnel: \"https://fcb2c5a30639.ngrok-free.app\" -> \"http://localhost:5000\"\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "from pyngrok import ngrok\n",
    "\n",
    "# Load model and processor once when app starts\n",
    "model_path = \"nanonets/Nanonets-OCR-s\"\n",
    "\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "processor = AutoProcessor.from_pretrained(model_path)\n",
    "\n",
    "# Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "def ocr_page_with_nanonets_s(image_path, model, processor, max_new_tokens=4096):\n",
    "    prompt = \"\"\"Extract the text from the above document as if you were reading it naturally. \n",
    "    Return the tables in html format. Return the equations in LaTeX representation. \n",
    "    If there is an image in the document and image caption is not present, \n",
    "    add a small description of the image inside the <img></img> tag; otherwise, add the image caption inside <img></img>. \n",
    "    Watermarks should be wrapped in brackets. Ex: <watermark>OFFICIAL COPY</watermark>. \n",
    "    Page numbers should be wrapped in brackets. Ex: <page_number>14</page_number> or <page_number>9/22</page_number>. \n",
    "    Prefer using ☐ and ☑ for check boxes.\"\"\"\n",
    "\n",
    "    image = Image.open(image_path)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"image\", \"image\": f\"file://{image_path}\"},\n",
    "            {\"type\": \"text\", \"text\": prompt},\n",
    "        ]},\n",
    "    ]\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = processor(text=[text], images=[image], padding=True, return_tensors=\"pt\")\n",
    "    inputs = inputs.to(model.device)\n",
    "    \n",
    "    output_ids = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n",
    "    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n",
    "    \n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "    )\n",
    "    return output_text[0]\n",
    "\n",
    "@app.route(\"/\")\n",
    "def home():\n",
    "    return \"Welcome to OCR LLM!\"\n",
    "\n",
    "\n",
    "@app.route(\"/ocr\", methods=[\"POST\"])\n",
    "def ocr_api():\n",
    "    if \"image\" not in request.files:\n",
    "        return jsonify({\"error\": \"No image uploaded\"}), 400\n",
    "    \n",
    "    image_file = request.files[\"image\"]\n",
    "    image_path = os.path.join(\"uploads\", image_file.filename)\n",
    "    os.makedirs(\"uploads\", exist_ok=True)\n",
    "    image_file.save(image_path)\n",
    "\n",
    "    try:\n",
    "        result = ocr_page_with_nanonets_s(image_path, model, processor, max_new_tokens=15000)\n",
    "        return jsonify({\"extracted_text\": result})\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "    finally:\n",
    "        if os.path.exists(image_path):\n",
    "            os.remove(image_path)  # cleanup temp file\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # app.run(host=\"0.0.0.0\", port=5000)\n",
    "    public_url = ngrok.connect(5000)\n",
    "    print(f\" * ngrok tunnel available at {public_url}\")\n",
    "    app.run(debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8387674,
     "sourceId": 13236249,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8387699,
     "sourceId": 13236289,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
